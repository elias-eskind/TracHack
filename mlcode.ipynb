{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TracHack Group 5\n",
    "### Cesar Diez, Elias Eskind, Austin Gravely, & Arlet Rodriguez\n",
    "#### 4/25/2021\n",
    "\n",
    "\n",
    "\n",
    "We used a gradient boosted tree classification model to predict one time redeemers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>148</td><td>application_1617643957232_0149</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-48-56.ec2.internal:20888/proxy/application_1617643957232_0149/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-53-91.ec2.internal:8042/node/containerlogs/container_1617643957232_0149_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the functions\n",
    "## Selecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Picking which sheets and which columns from those sheets we want to use\n",
    "def select_data(data_frame):\n",
    "    '''Selects and transforms the raw GCR (JSON) records data frame into a data frame.'''\n",
    "    \n",
    "    ##### Create UDFs\n",
    "    func_udf = func.udf(lambda X: \"empty\" if X=='' else X , StringType())\n",
    "    \n",
    "    ##### Adding columns\n",
    "    df_selection = data_frame.select('line_id',\"phone_info\", 'contact_info','loyalty-program', 'byop_phone_info', 'first_redemption_channel',\n",
    "                                     'first_redemption_date','carrier', \"network-usage-international\",\n",
    "                      func.explode_outer(\"network-usage-domestic\").alias('network-usage-domestic'))\n",
    " \n",
    "    ##### Have to do twice because of explode_outer\n",
    "    \n",
    "    df_selection = df_selection.select('line_id',\"phone_info.*\",'contact_info.*', 'loyalty-program.*', 'byop_phone_info.gsma_device_type', \n",
    "                                       'byop_phone_info.gsma_operating_system', func.col('byop_phone_info.manufacturer').alias('byop_manufacturer'), 'byop_phone_info.os_family',\n",
    "                                       'byop_phone_info.touch_screen', 'first_redemption_channel', 'first_redemption_date', 'carrier', \"network-usage-domestic\",\n",
    "                      func.explode_outer(\"network-usage-international\").alias('network-usage-international'))\n",
    "    \n",
    "    \n",
    "    ##### Final Selection\n",
    "    df_selection = df_selection.select('line_id',\"manufacturer\",\"model\",\"operating_system\",\"release_date\", # phone info\n",
    "                                       'extd_warranty', 'model_type', 'technology','language_preference', 'display_description', 'fm_radio', 'has_wifi_calling', 'mobile_hotspot', 'data_capable', # phone info\n",
    "                                       'opt_out_email', 'opt_out_mobiles_ads', 'opt_out_phone', 'state', 'lrp_enrolled', #contact info/loyalty program\n",
    "                                       'gsma_device_type', 'gsma_operating_system', 'byop_manufacturer','os_family', 'touch_screen', # byob\n",
    "                                       'first_redemption_channel', 'first_redemption_date', 'carrier', # other\n",
    "                      # Domestic \n",
    "                      func.col(\"network-usage-domestic.hotspot_kb\"),\n",
    "                      func.col(\"network-usage-domestic.total_kb\"),\n",
    "                      func.col(\"network-usage-domestic.voice_count_total\"),  \n",
    "                      func.col(\"network-usage-domestic.sms_in\"),\n",
    "                      func.col(\"network-usage-domestic.sms_out\"),\n",
    "                      func.col(\"network-usage-domestic.mms_in\"),\n",
    "                      func.col(\"network-usage-domestic.mms_out\"),\n",
    "                      #International\n",
    "                      func.col(\"network-usage-international.total_kb\").alias(\"int_total_kb\"),\n",
    "                      func.col(\"network-usage-international.voice_count_total\").alias(\"int_voice_count_total\"),  \n",
    "                      func.col(\"network-usage-international.hotspot_kb\").alias(\"int_hotspot_kb\"),  \n",
    "                      func.col(\"network-usage-international.sms_in\").alias(\"int_sms_in\"),\n",
    "                      func.col(\"network-usage-international.sms_out\").alias(\"int_sms_out\"),\n",
    "                      func.col(\"network-usage-international.mms_in\").alias(\"int_mms_in\"),\n",
    "                      func.col(\"network-usage-international.mms_out\").alias(\"int_mms_out\")).withColumn(\"language_preference\", func_udf(df_selection['language_preference']))\n",
    "    \n",
    "    \n",
    "    return df_selection\n",
    "\n",
    "# Get Usage Summary\n",
    "\n",
    "def get_usage_summary(df):\n",
    "    '''Returns aggregate voice count and data usage columns.'''\n",
    "    \n",
    "    ##### Adding summary variables for usuage (domestic and international)\n",
    "    \n",
    "    #Domestic\n",
    "    data_voice_count = df.select('line_id', \"voice_count_total\")\n",
    "    data_voice_count = data_voice_count.groupby('line_id').agg(func.sum('voice_count_total'))\\\n",
    "                        .selectExpr('line_id','`sum(voice_count_total)` as domestic_voice_count_total')\n",
    "    \n",
    "    data_hot = df.select('line_id','hotspot_kb')\n",
    "    data_hot = data_hot.groupby('line_id').agg(func.sum('hotspot_kb')).selectExpr('line_id',\n",
    "                    '`sum(hotspot_kb)` as domestic_data_hotspot_kb')\n",
    "    \n",
    "    data_kb = df.select('line_id','total_kb')\n",
    "    data_kb = data_kb.groupby('line_id').agg(func.sum('total_kb')).selectExpr('line_id',\n",
    "                    '`sum(total_kb)` as domestic_data_usage_kb')\n",
    "    \n",
    "    sms_in = df.select('line_id','sms_in')\n",
    "    sms_in = sms_in.groupby('line_id').agg(func.sum('sms_in')).selectExpr('line_id',\n",
    "                    '`sum(sms_in)` as domestic_sms_in')\n",
    "\n",
    "    sms_out = df.select('line_id','sms_out')\n",
    "    sms_out = sms_out.groupby('line_id').agg(func.sum('sms_out')).selectExpr('line_id',\n",
    "                    '`sum(sms_out)` as domestic_sms_out')\n",
    "    \n",
    "    mms_in = df.select('line_id','mms_in')\n",
    "    mms_in = mms_in.groupby('line_id').agg(func.sum('mms_in')).selectExpr('line_id',\n",
    "                    '`sum(mms_in)` as domestic_mms_in')\n",
    "    \n",
    "    mms_out = df.select('line_id','mms_out')\n",
    "    mms_out = mms_out.groupby('line_id').agg(func.sum('mms_out')).selectExpr('line_id',\n",
    "                    '`sum(mms_out)` as domestic_mms_out')\n",
    "    \n",
    "    # International\n",
    "    \n",
    "    int_voice_count_total = df.select('line_id','int_voice_count_total')\n",
    "    int_voice_count_total = int_voice_count_total.groupby('line_id').agg(func.sum('int_voice_count_total')).selectExpr('line_id',\n",
    "                    '`sum(int_voice_count_total)` as int_voice_count_total_sum')\n",
    "    \n",
    "    int_hotspot_kb = df.select('line_id','int_hotspot_kb')\n",
    "    int_hotspot_kb = int_hotspot_kb.groupby('line_id').agg(func.sum('int_hotspot_kb')).selectExpr('line_id',\n",
    "                    '`sum(int_hotspot_kb)` as int_hotspot_kb_sum')\n",
    "    \n",
    "    int_total_kb = df.select('line_id','int_total_kb')\n",
    "    int_total_kb = int_total_kb.groupby('line_id').agg(func.sum('int_total_kb')).selectExpr('line_id',\n",
    "                    '`sum(int_total_kb)` as int_total_kb_sum')\n",
    "    \n",
    "    int_sms_in = df.select('line_id','int_sms_in')\n",
    "    int_sms_in = int_sms_in.groupby('line_id').agg(func.sum('int_sms_in')).selectExpr('line_id',\n",
    "                    '`sum(int_sms_in)` as int_sms_in_sum')\n",
    "        \n",
    "    int_sms_out = df.select('line_id','int_sms_out')\n",
    "    int_sms_out = int_sms_out.groupby('line_id').agg(func.sum('int_sms_out')).selectExpr('line_id',\n",
    "                    '`sum(int_sms_out)` as int_sms_out_sum')\n",
    "    \n",
    "    int_mms_in = df.select('line_id','int_mms_in')\n",
    "    int_mms_in = int_mms_in.groupby('line_id').agg(func.sum('int_mms_in')).selectExpr('line_id',\n",
    "                    '`sum(int_mms_in)` as int_mms_in_sum')\n",
    "    \n",
    "    int_mms_out = df.select('line_id','int_mms_out')\n",
    "    int_mms_out = int_mms_out.groupby('line_id').agg(func.sum('int_mms_out')).selectExpr('line_id',\n",
    "                    '`sum(int_mms_out)` as int_mms_out_sum')\n",
    "    \n",
    "    #### Joining on line_id\n",
    "    usage_summary=data_voice_count.join(data_kb, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(data_hot, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(sms_in, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(sms_out, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(mms_in, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(mms_out, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(int_voice_count_total, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(int_hotspot_kb, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(int_total_kb, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(int_sms_in, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(int_sms_out, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(int_mms_in, on = 'line_id')\n",
    "    usage_summary=usage_summary.join(int_mms_out, on = 'line_id')\n",
    "    \n",
    "    \n",
    "    usage_summary= usage_summary.fillna(0)\n",
    "    \n",
    "    return usage_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# phone_info\n",
    "def preprocess_phone_info(df):\n",
    "    '''Imputes null values in manufacturer and model column with most frequent value.'''\n",
    "    # The pattern for the release date\n",
    "    pattern = \"M/d/yyyy\"\n",
    "    # Selecting what columns from phone_info we want\n",
    "    phone_info=df.select('line_id','manufacturer','model', 'operating_system', func.unix_timestamp(\"release_date\", pattern).alias(\"release_date\"),\n",
    "                         'extd_warranty', 'model_type', 'technology', 'display_description', \n",
    "                         'fm_radio', 'has_wifi_calling', 'mobile_hotspot', 'data_capable').dropDuplicates()\n",
    "    \n",
    "    # Finding the mode of certain columns so we can replace the null values with said mode\n",
    "    mode_manufacturer=phone_info.filter(func.col('manufacturer').isNotNull()).groupby('manufacturer').count().sort(func.col('count').desc()).first()['manufacturer']\n",
    "    mode_model=phone_info.filter(func.col('model').isNotNull()).groupby('model').count().sort(func.col('count').desc()).first()['model']\n",
    "    mode_display_description=phone_info.filter(func.col('display_description').isNotNull()).groupby('display_description').count().sort(func.col('count').desc()).first()['display_description']\n",
    "    \n",
    "    ### DATE STUFF ###\n",
    "    mode_date = phone_info.filter(func.col('release_date').isNotNull()).groupby('release_date').count().sort(func.col('count').desc()).first()['release_date']\n",
    "    ##################\n",
    "    \n",
    "    #Filling nulls\n",
    "    phone_info=phone_info.fillna({'manufacturer': mode_manufacturer,'model': mode_model, \n",
    "                                  'operating_system': \"Other\", 'release_date': mode_date, \n",
    "                                  'extd_warranty' : \"No\", 'model_type' : \"Non-Touch\", 'technology': \"Other\",\n",
    "                                  'display_description': mode_display_description, 'fm_radio': \"N\", 'has_wifi_calling': \"N\", \n",
    "                                  'mobile_hotspot': \"Y\", 'data_capable': 0})\n",
    "    return phone_info\n",
    "\n",
    "\n",
    "# contact_info\n",
    "def preprocess_contact_info(df):\n",
    "    \n",
    "    # Selecting what columns from contact_info we want\n",
    "    contact_info=df.select('line_id', 'language_preference', 'opt_out_email', 'opt_out_mobiles_ads', 'opt_out_phone', 'state').dropDuplicates()\n",
    "    # Dealing with the UDF for language we created earlier\n",
    "    contact_info = contact_info.withColumn(\"language_preference\", \\\n",
    "              func.when(contact_info[\"language_preference\"] == \"\", \"empty\").otherwise(contact_info[\"language_preference\"]))\n",
    "    \n",
    "    ##### Creating a new column for opt outs #####\n",
    "    contact_info = contact_info.withColumn(\n",
    "    'opt_out',\n",
    "    func.when((func.col(\"opt_out_email\") == 1) & (func.col(\"opt_out_mobiles_ads\")  == 1) & (func.col(\"opt_out_phone\") == 1), 'all')\\\n",
    "    .when((func.col(\"opt_out_email\") == 1) | (func.col(\"opt_out_mobiles_ads\")  == 1) & (func.col(\"opt_out_phone\") == 1), 'some1')\\\n",
    "    .when((func.col(\"opt_out_email\") == 1) & (func.col(\"opt_out_mobiles_ads\")  == 1) | (func.col(\"opt_out_phone\") == 1), 'some2')\\\n",
    "    .when((func.col(\"opt_out_email\") == 1) | (func.col(\"opt_out_mobiles_ads\")  == 1) | (func.col(\"opt_out_phone\") == 1), 'some3')\\\n",
    "    .when((func.col(\"opt_out_email\") == -1) & (func.col(\"opt_out_mobiles_ads\")  == -1) & (func.col(\"opt_out_phone\") == -1), 'none')\\\n",
    "    .when((func.col(\"opt_out_email\") == 'null') & (func.col(\"opt_out_mobiles_ads\")  == 'null') & (func.col(\"opt_out_phone\") == -1), 'none2')\\\n",
    "    .when((func.col(\"opt_out_email\") == 'null') & (func.col(\"opt_out_mobiles_ads\")  == -1) & (func.col(\"opt_out_phone\") == 'null'), 'none3')\\\n",
    "    .when((func.col(\"opt_out_email\") == -1) & (func.col(\"opt_out_mobiles_ads\")  == 'null') & (func.col(\"opt_out_phone\") == 'null'), 'none4')\n",
    "    .otherwise('none'))\n",
    "    \n",
    "    #### Dropping old ones\n",
    "    contact_info.drop(\"opt_out_email\",\"opt_out_mobiles_ads\",\"opt_out_phone\")\n",
    "    ##############################################\n",
    "    \n",
    "    # Mode for state to fill nulls\n",
    "    mode_state=contact_info.filter(func.col('state').isNotNull()).groupby('state').count().sort(func.col('count').desc()).first()['state']\n",
    "    \n",
    "    # Filling nulls\n",
    "    contact_info=contact_info.fillna({'language_preference':'EN','opt_out': 'null', 'state' : mode_state})\n",
    "    \n",
    "    return contact_info\n",
    "\n",
    "\n",
    "# byob_info\n",
    "def preprocess_byop_info(df):\n",
    "    \n",
    "    # Same as functions above\n",
    "    byop_info=df.select('line_id', 'gsma_device_type', 'gsma_operating_system', 'byop_manufacturer','os_family', 'touch_screen').dropDuplicates()\n",
    "\n",
    "    mode_gsma_device_type=byop_info.filter(func.col('gsma_device_type').isNotNull()).groupby('gsma_device_type').count().sort(func.col('count').desc()).first()['gsma_device_type']\n",
    "    mode_gsma_operating_system=byop_info.filter(func.col('gsma_operating_system').isNotNull()).groupby('gsma_operating_system').count().sort(func.col('count').desc()).first()['gsma_operating_system']\n",
    "    mode_byop_manufacturer=byop_info.filter(func.col('byop_manufacturer').isNotNull()).groupby('byop_manufacturer').count().sort(func.col('count').desc()).first()['byop_manufacturer']\n",
    "    mode_os_family=byop_info.filter(func.col('os_family').isNotNull()).groupby('os_family').count().sort(func.col('count').desc()).first()['os_family']  \n",
    "        \n",
    "    byop_info=byop_info.fillna({'gsma_device_type':mode_gsma_device_type, 'gsma_operating_system':mode_gsma_operating_system,'byop_manufacturer' : mode_byop_manufacturer,'os_family': mode_os_family, 'touch_screen': '1'})\n",
    "\n",
    "    return byop_info\n",
    "\n",
    "\n",
    "# loyalty_info\n",
    "def preprocess_loyalty_info(df):\n",
    "    loyalty_info=df.select('line_id', 'lrp_enrolled').dropDuplicates()\n",
    "    loyalty_info=loyalty_info.fillna({'lrp_enrolled':'empty'})\n",
    "\n",
    "    return loyalty_info\n",
    "\n",
    "\n",
    "# first_redemption_info\n",
    "def preprocess_remdemption_info(df):\n",
    "    \n",
    "    remdemption_info=df.select('line_id', 'first_redemption_channel').dropDuplicates()\n",
    "    remdemption_info=remdemption_info.fillna({'first_redemption_channel':'null'})\n",
    "\n",
    "    return remdemption_info\n",
    "\n",
    "\n",
    "# carrier_info\n",
    "def preprocess_carrier_info(df):\n",
    "    \n",
    "    carrier_info=df.select('line_id', 'carrier').dropDuplicates()\n",
    "    carrier_info=carrier_info.fillna({'carrier':'null'})\n",
    "\n",
    "    return carrier_info\n",
    "\n",
    "\n",
    "# first_redemption_date\n",
    "def preprocess_first_redemption_date(df):\n",
    "    \n",
    "    # Different pattern than last time\n",
    "    pattern = \"yyyy-MM-dd\"\n",
    "    redemption_date_info = df.select('line_id', func.unix_timestamp(\"first_redemption_date\", pattern).alias(\"first_redemption_date\")).dropDuplicates()\n",
    "    mode_redemption_date = redemption_date_info.filter(func.col('first_redemption_date').isNotNull()).groupby('first_redemption_date').count().sort(func.col('count').desc()).first()['first_redemption_date']\n",
    "    redemption_date_info = redemption_date_info.fillna({'first_redemption_date':mode_redemption_date})\n",
    "    \n",
    "    return redemption_date_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Featurized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fit_featurize_data(df, data_has_label=True,remove_orig_cols=True):\n",
    "    '''Given a selected data frame, generate a featurized dataframe.\n",
    "\n",
    "    Example: Shows how categorical features are handled by first a string indexer and then one-hot encoding.\n",
    "\n",
    "    if remove_orig_cols is set, only returns a dataframe with two columns - features and label.\n",
    "    '''\n",
    "    \n",
    "    ##### Categorical variables\n",
    "    categorical_columns = ['manufacturer','model', 'operating_system', 'language_preference', \n",
    "                           'extd_warranty', 'model_type', 'technology', 'opt_out', 'state', 'lrp_enrolled',  \n",
    "                          'gsma_device_type', 'gsma_operating_system', 'byop_manufacturer' , 'os_family', 'touch_screen', \n",
    "                           'first_redemption_channel', 'carrier',\n",
    "                          'display_description', 'fm_radio', 'has_wifi_calling', 'mobile_hotspot']\n",
    "    \n",
    "    ##### Numeric variables\n",
    "    numeric_columns = [\"domestic_data_usage_kb\", \"domestic_voice_count_total\", \"domestic_sms_in\", \"domestic_data_hotspot_kb\", \n",
    "                       \"domestic_sms_out\", \"domestic_mms_in\", \"domestic_mms_out\", 'int_voice_count_total_sum', 'int_hotspot_kb_sum',\n",
    "                       'int_total_kb_sum', 'int_sms_in_sum', 'int_sms_out_sum', 'int_mms_in_sum', 'int_mms_out_sum', 'release_date',\n",
    "                       'data_capable','first_redemption_date']\n",
    "    \n",
    "    # filling in any remaining nulls\n",
    "    df = df.na.fill(\"NA\", subset = categorical_columns)\n",
    "    df = df.na.fill(0, subset = numeric_columns)    \n",
    "    \n",
    "    stages = []\n",
    "    for column in categorical_columns:\n",
    "        string_indexer = StringIndexer(inputCol = column, handleInvalid=\"keep\",outputCol = column + '_index')\n",
    "        encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[column + \"_vec\"])\n",
    "        stages += [string_indexer, encoder]\n",
    "    \n",
    "    label_indexer = StringIndexer(inputCol =\"one_time_redeemer\", outputCol=\"label\",handleInvalid=\"keep\")\n",
    "    stages += [label_indexer]\n",
    "        \n",
    "    assembler_inputs = [c + \"_vec\" for c in categorical_columns] + numeric_columns\n",
    "    assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    \n",
    "    return pipeline_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_balancing_ratio(data_frame, target_var='label'):\n",
    "    \"\"\"Calculates the class ratio of label vs non-label in given dataframe\n",
    "\n",
    "    :param data_frame: dataframe with GCR records.\n",
    "    :type data_frame: Dataframe.\n",
    "    :param target_var: Name of label column.\n",
    "    :type target_var: str.\n",
    "    :returns: ratio of label vs non-label in input dataframe.\n",
    "    :rtype: int.\n",
    "    \"\"\"\n",
    "    active_count = data_frame.filter(func.col(target_var) == 0.0).count()\n",
    "    total_count = data_frame.count()\n",
    "    balancing_ratio = active_count / total_count\n",
    "    return balancing_ratio\n",
    "\n",
    "def train_model(train_dataset):\n",
    "    '''Given a featurized training dataset, trains a simple logistic regression model and \n",
    "    returns the trained model object'''\n",
    "    negative_ratio = np.round(get_balancing_ratio(train_dataset), 2)\n",
    "    train_dataset = train_dataset.withColumn(\"classWeights\",func.when(train_dataset['label'] == 1, negative_ratio).\\\n",
    "                                             otherwise(\n",
    "                                            1 - negative_ratio))\n",
    "    \n",
    "    # Gradient Boosted Tree Classifier\n",
    "    from pyspark.ml.classification import GBTClassifier\n",
    "    gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", weightCol='classWeights',  maxIter=10)\n",
    "    \n",
    "    from pyspark.ml import Pipeline\n",
    "    pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "    model = pipeline.fit(train_dataset)\n",
    "    prediction = model.transform(train_dataset)\n",
    "    prediction.printSchema()\n",
    "\n",
    "    # Evaluating it (not sure if necessary)\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "    \n",
    "    binEval.evaluate(prediction)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_dataset):\n",
    "    '''Given a model and featurized test dataset, returns the f1 value'''\n",
    "    predictions = model.transform(test_dataset).select(['prediction', 'label'])\n",
    "    metrics = MulticlassMetrics(predictions.rdd)\n",
    "    if predictions.select('label').distinct().count()==2:\n",
    "        f1 = metrics.fMeasure(label = 1.0)\n",
    "    else: f1 = 0\n",
    "    return round(f1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_predictions(pipeline_model, model,eval_data_path,submission_path):\n",
    "    '''Given a model, eval data path and submission path, makes predictions and \n",
    "saves the submissions to submission path.'''\n",
    "    # Getting data from the evaluation file\n",
    "    eval_data = spark.read.json(eval_data_path)\n",
    "    \n",
    "    # Getting columns\n",
    "    df_selected = select_data(eval_data)\n",
    "    usage_summary=get_usage_summary(df_selected)\n",
    "    \n",
    "    # Preprocessing\n",
    "    phone_info=preprocess_phone_info(df_selected)\n",
    "    contact_info=preprocess_contact_info(df_selected)\n",
    "    loyalty_info=preprocess_loyalty_info(df_selected)\n",
    "    byop_info=preprocess_byop_info(df_selected)\n",
    "    redemption_info=preprocess_remdemption_info(df_selected)\n",
    "    carrier_info=preprocess_carrier_info(df_selected)\n",
    "    redemption_date_info=preprocess_first_redemption_date(df_selected)\n",
    "    line_ids=df_selected.select('line_id').dropDuplicates()\n",
    "    \n",
    "    #Joining processed data\n",
    "    df_preprocessed=line_ids.join(usage_summary,on='line_id', how = 'full')\n",
    "    df_preprocessed=df_preprocessed.join(phone_info,on='line_id', how = 'full')\n",
    "    df_preprocessed=df_preprocessed.join(contact_info,on='line_id', how = 'full')\n",
    "    df_preprocessed=df_preprocessed.join(loyalty_info,on='line_id', how = 'full')\n",
    "    df_preprocessed=df_preprocessed.join(byop_info,on='line_id', how = 'full')\n",
    "    df_preprocessed=df_preprocessed.join(redemption_info,on='line_id', how = 'full')\n",
    "    df_preprocessed=df_preprocessed.join(carrier_info,on='line_id', how = 'full')\n",
    "    df_preprocessed=df_preprocessed.join(redemption_date_info,on='line_id', how = 'full') \n",
    "    \n",
    "\n",
    "    categorical_columns = ['manufacturer','model', 'operating_system', 'language_preference', \n",
    "                           'extd_warranty', 'model_type', 'technology', 'opt_out', 'state', 'lrp_enrolled',  \n",
    "                          'gsma_device_type', 'gsma_operating_system', 'byop_manufacturer' , 'os_family', 'touch_screen', \n",
    "                           'first_redemption_channel', 'carrier',\n",
    "                          'display_description', 'fm_radio', 'has_wifi_calling', 'mobile_hotspot']\n",
    "    numeric_columns = [\"domestic_data_usage_kb\", \"domestic_voice_count_total\", \"domestic_sms_in\", \"domestic_data_hotspot_kb\", \n",
    "                       \"domestic_sms_out\", \"domestic_mms_in\", \"domestic_mms_out\", 'int_voice_count_total_sum', 'int_hotspot_kb_sum',\n",
    "                       'int_total_kb_sum', 'int_sms_in_sum', 'int_sms_out_sum', 'int_mms_in_sum', 'int_mms_out_sum', 'release_date',\n",
    "                       'data_capable','first_redemption_date'] \n",
    "\n",
    "    df_preprocessed = df_preprocessed.na.fill(\"NA\", subset = categorical_columns)\n",
    "    df_preprocessed = df_preprocessed.na.fill(0, subset = numeric_columns)   \n",
    "    df_featurized = pipeline_model.transform(df_preprocessed)\n",
    "\n",
    "    # Do you want to remove the original columns?  I dooooo\n",
    "    remove_orig_cols = True\n",
    "    if remove_orig_cols:\n",
    "        selected_columns = ['line_id','features']\n",
    "        df_featurized = df_featurized.select(selected_columns)\n",
    "\n",
    "    # Predicting the one-time-redeemers for the evaluation dataset\n",
    "    predictions=model.transform(df_featurized).select(['line_id', 'prediction'])\n",
    "    predictions=predictions.selectExpr('line_id','prediction as one_time_redeeemer')\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_folder='s3://tf-trachack-data/211/data/'\n",
    "# This path will be active the launch of the hackathon\n",
    "teamname = 'trachack-project-groups-5-umiami'\n",
    "root_folder='s3://tf-trachack-notebooks/trachack-project-groups-5-umiami/jupyter/jovyan/'\n",
    "\n",
    "spark = SparkSession.builder.appName('trachack-code-submission').enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main()\n",
    "#### Doing it like this because if it is all in one function it takes > 60 minutes and times out\n",
    "(we could always increase the timeout time in terminal but whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 3.0 minutes & 19.82 seconds"
     ]
    }
   ],
   "source": [
    "##TIMER##\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "#########\n",
    "\n",
    "# Getting datapath\n",
    "data_path = data_folder + \"dev.json.bz2\"\n",
    "\n",
    "# Getting data from s3\n",
    "df = spark.read.json(data_path)  \n",
    "\n",
    "# Selecting which columns we want\n",
    "df_selected = select_data(df)\n",
    "usage_summary=get_usage_summary(df_selected)\n",
    "\n",
    "# Preprocessing\n",
    "phone_info=preprocess_phone_info(df_selected)\n",
    "contact_info=preprocess_contact_info(df_selected)\n",
    "loyalty_info=preprocess_loyalty_info(df_selected)\n",
    "byop_info=preprocess_byop_info(df_selected)\n",
    "redemption_info=preprocess_remdemption_info(df_selected)\n",
    "carrier_info=preprocess_carrier_info(df_selected)\n",
    "redemption_date_info=preprocess_first_redemption_date(df_selected)\n",
    "\n",
    "# Joining on line_id\n",
    "line_ids=df.select('line_id','one_time_redeemer').dropDuplicates()\n",
    "df_preprocessed=line_ids.join(usage_summary,on='line_id', how = 'full')\n",
    "df_preprocessed=df_preprocessed.join(phone_info,on='line_id', how = 'full')\n",
    "df_preprocessed=df_preprocessed.join(contact_info,on='line_id', how = 'full')\n",
    "df_preprocessed=df_preprocessed.join(loyalty_info,on='line_id', how = 'full')\n",
    "df_preprocessed=df_preprocessed.join(byop_info,on='line_id', how = 'full')\n",
    "df_preprocessed=df_preprocessed.join(redemption_info,on='line_id', how = 'full')\n",
    "df_preprocessed=df_preprocessed.join(carrier_info,on='line_id', how = 'full') \n",
    "df_preprocessed=df_preprocessed.join(redemption_date_info,on='line_id', how = 'full')\n",
    "\n",
    "# How long did it take?\n",
    "stop = timeit.default_timer()\n",
    "print('Runtime:', (stop - start)//60, 'minutes &', round(((stop - start)%60),2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 42.0 minutes & 47.95 seconds"
     ]
    }
   ],
   "source": [
    "# Seperate chunks because this chunk takes ~45 minutes\n",
    "start = timeit.default_timer()\n",
    "pipeline_model = fit_featurize_data(df_preprocessed)\n",
    "df_featurized = pipeline_model.transform(df_preprocessed)\n",
    "\n",
    "remove_orig_cols=True\n",
    "if remove_orig_cols:\n",
    "    selected_columns = ['features', 'label']\n",
    "    df_featurized= df_featurized.select(selected_columns)\n",
    "\n",
    "df_featurized.cache()\n",
    "stop = timeit.default_timer()\n",
    "print('Runtime:', (stop - start)//60, 'minutes &', round(((stop - start)%60),2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train has 69856\n",
      "Test has 3703\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- classWeights: double (nullable = false)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "Runtime: 9.0 minutes & 24.99 seconds\n",
      "f1-score: 0.5048"
     ]
    }
   ],
   "source": [
    "# Creating the model and testing it\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Selecting a seed for reproducibility \n",
    "random_seed = 69 # we're mature I swear\n",
    "\n",
    "# The ratio we want training to testing to be\n",
    "test_ratio = .05\n",
    "\n",
    "train, test = df_featurized.randomSplit([1.0 - test_ratio , test_ratio], seed = random_seed)\n",
    "\n",
    "num_train = train.count()\n",
    "num_test = test.count()\n",
    "print(f\"Train has {num_train}\")\n",
    "print(f\"Test has {num_test}\")\n",
    "\n",
    "# Create model\n",
    "model = train_model(train)\n",
    "\n",
    "# Our f1 score (evaluation of model)\n",
    "f1 = evaluate_model(model, test)\n",
    "\n",
    "#How long did it take?\n",
    "stop = timeit.default_timer()\n",
    "print('Runtime:', (stop - start)//60, 'minutes &', round(((stop - start)%60),2), 'seconds')\n",
    "\n",
    "# How'd we do?\n",
    "print(f\"f1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Our Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------+\n",
      "|line_id                             |one_time_redeeemer|\n",
      "+------------------------------------+------------------+\n",
      "|00035778-3662-4112-9c68-8993a4d20ca9|0.0               |\n",
      "|008b9332-d302-446f-aff2-b6317bda785e|0.0               |\n",
      "|0399fe7e-2f11-48f1-bad3-aa30682f3f05|0.0               |\n",
      "|0595432d-b946-4e99-9c7d-eb6eb58711ea|1.0               |\n",
      "|06e0b271-ca1b-4d41-85ef-569a0b33b2cd|0.0               |\n",
      "+------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Runtime: 6.0 minutes & 28.82 seconds"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "eval_data_path=data_folder+'eval.json.bz2'\n",
    "submission_path=root_folder+\"submission/2021-04-25/\" #Change the date to whatever is required\n",
    "\n",
    "predictions = make_predictions(pipeline_model, model,eval_data_path,submission_path).cache()\n",
    "\n",
    "# Does it looks right?\n",
    "predictions.show(5,truncate=False)\n",
    "stop = timeit.default_timer()\n",
    "print('Runtime:', (stop - start)//60, 'minutes &', round(((stop - start)%60),2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission saved to s3://tf-trachack-notebooks/trachack-project-groups-5-umiami/jupyter/jovyan/submission/2021-04-25/\n",
      "Runtime: 0.0 minutes & 29.98 seconds"
     ]
    }
   ],
   "source": [
    "# If so then lets submit it\n",
    "start = timeit.default_timer()\n",
    "predictions.write.mode('overwrite').option('header',True).csv(submission_path)\n",
    "print(f\"submission saved to {submission_path}\")\n",
    "\n",
    "# We like timing things\n",
    "stop = timeit.default_timer()\n",
    "print('Runtime:', (stop - start)//60, 'minutes &', round(((stop - start)%60),2), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making sure we have 100% coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49231"
     ]
    }
   ],
   "source": [
    "num_eval=spark.read.option('header',True).csv(eval_data_path)\n",
    "num_eval.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 1.0"
     ]
    }
   ],
   "source": [
    "num_predictions = predictions.count()\n",
    "coverage = num_predictions / 49232\n",
    "\n",
    "# If this = 1 then we are at 100% coverage\n",
    "print(f\"Coverage: {coverage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yay!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
